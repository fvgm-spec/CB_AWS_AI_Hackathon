{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "jj4Lsm_y2quu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch"
      ],
      "metadata": {
        "id": "GKTBczhK234y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKRvPu4EmhiF",
        "outputId": "136330ff-94dd-4c78-a35b-a4850757a486"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Epoch 1/10: 0it [00:00, ?it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 1/10: 6it [02:43, 26.90s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 1/10: 10it [04:33, 27.41s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 1/10: 20it [09:10, 27.66s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 1/10: 22it [10:04, 27.47s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 1/10: 27it [12:21, 27.43s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 1/10: 31it [14:13, 28.08s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 1/10: 33it [15:09, 27.90s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 1/10: 43it [19:47, 27.63s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Average Loss: -0.2308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 4it [01:49, 27.41s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 2/10: 11it [05:03, 27.56s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 2/10: 14it [06:25, 27.40s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 2/10: 17it [07:48, 27.63s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 2/10: 26it [11:56, 27.66s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 2/10: 31it [14:12, 27.31s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 2/10: 32it [14:40, 27.59s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 2/10: 34it [15:34, 27.37s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 2/10: 43it [19:42, 27.50s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10 - Average Loss: -0.8369\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 1it [00:25, 25.73s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 3/10: 3it [01:22, 27.77s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 3/10: 4it [01:48, 27.08s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 3/10: 14it [06:26, 27.83s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 3/10: 18it [08:17, 27.81s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 3/10: 22it [10:06, 27.37s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 3/10: 30it [13:55, 28.16s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 3/10: 42it [19:26, 27.78s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 3/10: 43it [19:53, 27.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10 - Average Loss: -1.3652\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 7it [03:12, 27.93s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 4/10: 9it [04:07, 27.64s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 4/10: 16it [07:21, 27.98s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 4/10: 26it [11:57, 27.60s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 4/10: 29it [13:21, 27.59s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 4/10: 35it [16:07, 27.20s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 4/10: 38it [17:31, 27.54s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 4/10: 42it [19:23, 27.91s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 4/10: 43it [19:51, 27.72s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10 - Average Loss: -1.9594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 3it [01:22, 27.86s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 5/10: 7it [03:11, 27.14s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 5/10: 8it [03:40, 27.69s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 5/10: 15it [06:56, 28.32s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 5/10: 19it [08:48, 27.93s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 5/10: 24it [11:08, 28.21s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 5/10: 27it [12:31, 27.91s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 5/10: 31it [14:19, 27.09s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 5/10: 43it [19:49, 27.67s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10 - Average Loss: -2.5549\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 7it [03:15, 27.79s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 6/10: 8it [03:43, 27.89s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 6/10: 10it [04:39, 27.96s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 6/10: 14it [06:27, 27.32s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 6/10: 22it [10:07, 27.20s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 6/10: 34it [15:37, 27.48s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 6/10: 37it [17:00, 27.31s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 6/10: 41it [18:53, 27.86s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 6/10: 43it [19:46, 27.60s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10 - Average Loss: -3.1286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 0it [00:00, ?it/s]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 7/10: 14it [06:26, 27.29s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 7/10: 20it [09:15, 27.84s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 7/10: 23it [10:36, 27.27s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 7/10: 32it [14:44, 27.49s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 7/10: 34it [15:40, 27.73s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 7/10: 43it [19:51, 27.70s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10 - Average Loss: -3.7478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 14it [06:28, 27.70s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 8/10: 17it [07:49, 27.42s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 8/10: 26it [11:59, 28.11s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 8/10: 33it [15:13, 27.69s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 8/10: 36it [16:35, 27.22s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 8/10: 38it [17:29, 27.16s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 8/10: 39it [17:56, 27.00s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 8/10: 43it [19:49, 27.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10 - Average Loss: -4.2926\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 3it [01:22, 27.63s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 9/10: 4it [01:48, 27.04s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 9/10: 14it [06:26, 27.82s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 9/10: 21it [09:40, 27.62s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 9/10: 27it [12:27, 28.01s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 9/10: 32it [14:46, 28.21s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 9/10: 36it [16:37, 27.75s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 9/10: 40it [18:28, 27.80s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 9/10: 43it [19:53, 27.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10 - Average Loss: -4.7837\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 2it [00:57, 28.81s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 10/10: 4it [01:53, 28.33s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 10/10: 6it [02:49, 28.12s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 10/10: 18it [08:24, 27.81s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 10/10: 28it [13:02, 28.06s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 10/10: 36it [16:46, 27.92s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 10/10: 40it [18:37, 27.91s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 10/10: 42it [19:32, 27.70s/it]Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Epoch 10/10: 43it [20:00, 27.91s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10 - Average Loss: -5.2776\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('trained_model/tokenizer_config.json',\n",
              " 'trained_model/special_tokens_map.json',\n",
              " 'trained_model/vocab.txt',\n",
              " 'trained_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the path to your CSV file\n",
        "csv_file = '/content/AWS Project Dataset.csv'\n",
        "\n",
        "# Define the number of training epochs and batch size\n",
        "epochs = 10\n",
        "batch_size = 4\n",
        "accumulation_steps = 8  # Number of steps before backward pass and parameter update\n",
        "\n",
        "# Load the data from the CSV file\n",
        "data = pd.read_csv(csv_file, encoding='latin-1')\n",
        "\n",
        "# Preprocessing\n",
        "data = data.dropna()  # Drop rows with missing values\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define a custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        question = row['Question']\n",
        "        answer = row['Answer']\n",
        "\n",
        "        # Tokenize the input text\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            question,\n",
        "            answer,\n",
        "            add_special_tokens=True,\n",
        "            max_length=512,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = inputs['input_ids'].squeeze()\n",
        "        attention_mask = inputs['attention_mask'].squeeze()\n",
        "\n",
        "        return input_ids, attention_mask\n",
        "\n",
        "# Create an instance of the custom dataset\n",
        "dataset = CustomDataset(data, tokenizer)\n",
        "\n",
        "# Create a data loader\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Initialize the BERT model for sequence classification\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "# Training loop\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "accumulated_loss = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "\n",
        "    for i, batch in tqdm(enumerate(dataloader), desc=f'Epoch {epoch + 1}/{epochs}'):\n",
        "        input_ids, attention_mask = batch\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=None)\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Calculate mean loss across the batch\n",
        "        loss = loss.mean()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        accumulated_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "    average_loss = accumulated_loss / len(dataloader)\n",
        "    print(f'Epoch {epoch + 1}/{epochs} - Average Loss: {average_loss:.4f}')\n",
        "    accumulated_loss = 0\n",
        "\n",
        "# Save the trained model\n",
        "model.save_pretrained('trained_model')\n",
        "tokenizer.save_pretrained('trained_model')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "\n",
        "import torch\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model_name = \"facebook/bart-base\"\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "questions = [\"Where is the German embassy in Mumbai?\", \"Where is the city office in Berlin?\", \"Where is the foreigners office in Berlin?\"]\n",
        "answers = [\"The German embassy is located at House, 9th Floor, Hoechst, 193, Backbay Reclamation, Nariman Point, Mumbai, Maharashtra 400021, India. Contact Number: +91 22 2283 2422. \", \"In Berlin, the city office is located at Kurfuerstendamm 194, 10707 Berlin. Contact Number: 030 700159829\", \"In Berlin, the foreigners office is located at Stuttgarter Str. 54, 12059 Berlin. Contact Number: 030 61642707\"]\n",
        "\n",
        "# Prepare the data for the BART model\n",
        "data = []\n",
        "for question, answer in zip(questions, answers):\n",
        "    data.append(\n",
        "        tokenizer(\n",
        "            \"question: %s  context: %s </s>\" % (question, answer),\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "inputs = torch.cat([x['input_ids'] for x in data])\n",
        "attention_masks = torch.cat([x['attention_mask'] for x in data])\n",
        "\n",
        "dataset = torch.utils.data.TensorDataset(inputs, attention_masks)\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 16\n",
        "num_epochs = 3\n",
        "learning_rate = 2e-5\n",
        "\n",
        "# Create a data loader for training\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Create an optimizer and set the learning rate\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, attention_mask = batch\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Average Loss: {average_loss:.4f}\")\n",
        "\n",
        "# Example evaluation\n",
        "eval_texts = [\"Can you tell me the address of the German embassy in Mumbai?\", \"Where can I find the city office in Berlin?\"]\n",
        "eval_answers = [\"The German embassy is located at House, 9th Floor, Hoechst, 193, Backbay Reclamation, Nariman Point, Mumbai, Maharashtra 400021, India. Contact Number: +91 22 2283 2422. \", \"In Berlin, the city office is located at Kurfuerstendamm 194, 10707 Berlin. Contact Number: 030 700159829\"]\n",
        "\n",
        "eval_data = []\n",
        "for eval_text, eval_answer in zip(eval_texts, eval_answers):\n",
        "    eval_data.append(\n",
        "        tokenizer(\n",
        "            \"question: %s  context: %s </s>\" % (eval_text, eval_answer),\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "eval_inputs = torch.cat([x['input_ids'] for x in eval_data])\n",
        "eval_attention_masks = torch.cat([x['attention_mask'] for x in eval_data])\n",
        "\n",
        "eval_dataset = torch.utils.data.TensorDataset(eval_inputs, eval_attention_masks)\n",
        "\n",
        "eval_loader = torch.utils.data.DataLoader(eval_dataset, batch_size=batch_size)\n",
        "\n",
        "model.eval()\n",
        "total_correct = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in eval_loader:\n",
        "        input_ids, attention_mask = batch\n",
        "\n",
        "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=512)\n",
        "        predicted_answers = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "        for predicted_answer in predicted_answers:\n",
        "            print(\"Predicted Answer:\", predicted_answer)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "brtpEY7HDF71",
        "outputId": "28ae3457-6b49-4815-8caa-ae4e265d91cf"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Epoch 1/3 - Average Loss: 15.7909\n",
            "Epoch 2/3 - Average Loss: 14.1386\n",
            "Epoch 3/3 - Average Loss: 13.2865\n",
            "Predicted Answer: question: Can you tell me the address of the German embassy in Mumbai?  context: The German embassy is located at House, 9th Floor, Hoechst, 193, Backbay Reclamation, Nariman Point, Mumbai, Maharashtra 400021, India. Contact Number: +91 22 2283 2422.  \n",
            "Predicted Answer: question: Where can I find the city office in Berlin?  context: In Berlin? The city office is located at Kurfuerstendamm 194, 10707 Berlin. Contact Number: 030 700159829 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "# Load the pre-trained model and tokenizer\n",
        "model_name = \"facebook/bart-base\"\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "questions = [\"Where is the German embassy in Mumbai?\", \"Where is the city office in Berlin?\", \"Where is the foreigners office in Berlin?\"]\n",
        "answers = [\"The German embassy is located at House, 9th Floor, Hoechst, 193, Backbay Reclamation, Nariman Point, Mumbai, Maharashtra 400021, India. Contact Number: +91 22 2283 2422. \", \"In Berlin, the city office is located at Kurfuerstendamm 194, 10707 Berlin. Contact Number: 030 700159829\", \"In Berlin, the foreigners office is located at Stuttgarter Str. 54, 12059 Berlin. Contact Number: 030 61642707\"]\n",
        "\n",
        "# Prepare the data for the BART model\n",
        "data = []\n",
        "for question, answer in zip(questions, answers):\n",
        "    data.append(\n",
        "        tokenizer(\n",
        "            \"question: %s  context: %s </s>\" % (question, answer),\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "inputs = torch.cat([x['input_ids'] for x in data])\n",
        "attention_masks = torch.cat([x['attention_mask'] for x in data])\n",
        "\n",
        "dataset = torch.utils.data.TensorDataset(inputs, attention_masks)\n",
        "\n",
        "# Define training parameters\n",
        "batch_size = 16\n",
        "num_epochs = 3\n",
        "learning_rate = 2e-5\n",
        "\n",
        "# Create a data loader for training\n",
        "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Create an optimizer and set the learning rate\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, attention_mask = batch\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Average Loss: {average_loss:.4f}\")\n",
        "\n",
        "# Example evaluation\n",
        "eval_texts = [\"Can you tell me the address of the German embassy in Mumbai?\", \"Where can I find the city office in Berlin?\"]\n",
        "eval_answers = [\"The German embassy is located at House, 9th Floor, Hoechst, 193, Backbay Reclamation, Nariman Point, Mumbai, Maharashtra 400021, India. Contact Number: +91 22 2283 2422. \", \"In Berlin, the city office is located at Kurfuerstendamm 194, 10707 Berlin. Contact Number: 030 700159829\"]\n",
        "\n",
        "eval_data = []\n",
        "for eval_text, eval_answer in zip(eval_texts, eval_answers):\n",
        "    eval_data.append(\n",
        "        tokenizer(\n",
        "            \"question: %s  context: %s </s>\" % (eval_text, eval_answer),\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "    )\n",
        "\n",
        "eval_inputs = torch.cat([x['input_ids'] for x in eval_data])\n",
        "eval_attention_masks = torch.cat([x['attention_mask'] for x in eval_data])\n",
        "\n",
        "eval_dataset = torch.utils.data.TensorDataset(eval_inputs, eval_attention_masks)\n",
        "eval_loader = torch.utils.data.DataLoader(eval_dataset, batch_size=batch_size)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print(\"Question                     Answer\")\n",
        "print(\"---------------------------------------\")\n",
        "with torch.no_grad():\n",
        "    for batch in eval_loader:\n",
        "        input_ids, attention_mask = batch\n",
        "\n",
        "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=512)\n",
        "        predicted_answers = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "        for eval_text, predicted_answer in zip(eval_texts, predicted_answers):\n",
        "            print(eval_text)\n",
        "            print('/n',predicted_answer)\n",
        "            print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frc7VjtkDfUM",
        "outputId": "dad44fd5-efb4-4104-b678-42f3d700c3cc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3 - Average Loss: 16.0992\n",
            "Epoch 2/3 - Average Loss: 13.9929\n",
            "Epoch 3/3 - Average Loss: 13.0815\n",
            "Question                     Answer\n",
            "---------------------------------------\n",
            "Can you tell me the address of the German embassy in Mumbai?\n",
            "/n question: Can you tell me the address of the German embassy in Mumbai?  context: The German embassy is located at House, 9th Floor, Hoechst, 193, Backbay Reclamation, Nariman Point, Mumbai, Maharashtra 400021, India. Contact Number: +91 22 2283 2422.  \n",
            "\n",
            "Where can I find the city office in Berlin?\n",
            "/n question: Where can I find the city office in Berlin?  context: In Berlin, the city offices is located at Kurfuerstendamm 194, 10707 Berlin. Contact Number: 030 700159829 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J2pj8vDUEfUF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}